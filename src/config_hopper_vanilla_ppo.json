{
    "adam_eps": 1e-05,
    "advanced_logging": false,
    "anneal_lr": true,
    "attack_eps": "same",
    "attack_method": "none",
    "attack_ratio": 1.0,
    "attack_sarsa_action_ratio": 0.5,
    "attack_sarsa_network": "sarsa_hopper_vanilla.model",
    "attack_step_eps": "auto",
    "attack_steps": 10,
    "cg_steps": 10,
    "clip_eps": 0.2,
    "clip_grad_norm": -1,
    "clip_observations": 10.0,
    "clip_rewards": 10.0,
    "clip_val_eps": 0.2,
    "cpu": true,
    "damping": 0.1,
    "entropy_coeff": 0.0,
    "fisher_frac_samples": 0.1,
    "game": "Hopper-v2",
    "gamma": 0.99,
    "initialization": "orthogonal",
    "kl_approximation_iters": -1,
    "lambda": 0.95,
    "log_every": 1,
    "max_backtrack": 10,
    "max_kl": 0.01,
    "max_kl_final": 0.01,
    "mode": "ppo",
    "norm_rewards": "returns",
    "norm_states": true,
    "num_actors": 1,
    "num_minibatches": 32,
    "out_dir": "vanilla_ppo_hopper/agents",
    "policy_activation": "tanh",
    "policy_net_type": "CtsPolicy",
    "ppo_epochs": 10,
    "ppo_lr": -1,
    "ppo_lr_adam": 0.0003,
    "robust_ppo_beta": 1.0,
    "robust_ppo_beta_scheduler_opts": "same",
    "robust_ppo_detach_stdev": false,
    "robust_ppo_eps": 0.075,
    "robust_ppo_eps_scheduler_opts": "start=1,length=732",
    "robust_ppo_method": "convex-relax",
    "robust_ppo_pgd_steps": 10,
    "robust_ppo_reg": 0.03,
    "save_iters": 10,
    "save_frames": false,
    "save_frames_path": "frames/",
    "share_weights": false,
    "show_env": false,
    "t": 2048,
    "train_steps": 976,
    "trpo_kl_reduce_func": "mean",
    "val_epochs": 10,
    "val_lr": 0.00025,
    "value_calc": "gae",
    "value_clipping": false,
    "value_multiplier": 0.1,
    "value_net_type": "ValueNet"
}
